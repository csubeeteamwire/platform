[all]
# When the host names cannot be resolved, add "ansible_host=<IP ADDRESS>" with
# the respective IP address of the individual machine to the host definitions.
lb1  private_ip=10.0.1.3
lb2  private_ip=10.0.1.4
tw1  private_ip=10.0.1.31
tw2  private_ip=10.0.1.32
tw3  private_ip=10.0.1.33
tw4  private_ip=10.0.1.34
tw5  private_ip=10.0.1.35
core private_ip=10.0.1.10
ctl1 private_ip=10.0.1.11
ctl2 private_ip=10.0.1.12
ctl3 private_ip=10.0.1.13
db1  private_ip=10.0.1.21
db2  private_ip=10.0.1.22
db3  private_ip=10.0.1.23

[all:vars]
# Optional parameters for the ssh client
# ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
# The user name used to connect to the given machines
# ansible_user=teamwire
# The private key of the user given above
# ansible_ssh_private_key_file=/path/to/private/key


# The following blocks configure which of the roles will be installed on which
# of the machines defined in the [all] section above. The comments above the
# blocks explain if the role is optional or mandatory for a clustered setup.

# Required. At least 1 management server required. This is the role which
# will handle Ansible provisioning and twctl management
[management_servers]
core

# Mandatory, three or five servers.
# This group of servers will host the Hashicorp stack in server-mode (Consul, Nomad and Vault).
[hashi_servers]
ctl1
ctl2
ctl3

# Three servers are required.
[redis_servers]
db1
db2
db3

# Optional. If you run a MySQL or Oracle database you can use that.
# When installing the database as part of the cluster, three or five
# servers are required.
[database_servers]
db1
db2
db3

# Define the servers which you want to act as storage server (glusterfs) nodes.
# You may comment out the storage_servers group and server list if you are going
# to use an external fileshare server, please check the file storage section of the
# all.example file for more information on this.
[storage_servers]
db1
db2
db3

# Optional, but recommended to reduce WAN traffic when pulling new Teamwire versions.
[docker_registry]
core

# Define where logs generated by all docker containers are sent to.
# Optional but heavily recommended. Multiple targets are supported.
[loghost]
core

# Required. Choose a number of servers that can handle the load generated by
# your client applications.
[backend_servers]
tw1
tw2
tw3
tw4
tw5

# Optional. This section is only required when running voip. Ensure also that variable
# "enable_voip" is set to "true" with quotes in group_vars/all file. Please also remember
# that each registered server may require a port share in the firewall on port 10000
#[voip_servers]
#vp1

# Optional. This section is only used when running video in cluster mode. Ensure also that variable
# "enable_voip" is set to "true" with quotes in group_vars/all file. Please also remember
# that each registered servers may require a port share in the firewall on port 10000
# This server group, defines where VoipVideo container will run.
# This section is only used if both enable_voip is enabled and enable_jvb_cluster is enabled.
#[video_servers]
#tw1
#tw2
#tw3

# Required. This role can be installed on the same servers as the backend
# when an existing load balancer is used.
[frontend_servers]
lb1
lb2

# Optional. This role allows you to expose a single IP address (configured in
# the section below) to the client application while maintaining redundancy.
[load_balancers]
lb1
lb2

[load_balancers:vars]
loadbalancer_external_ip=10.0.1.2
loadbalancer_password="12secr34"

# Optional. This role allows you to configure on-premise monitoring.
# Here you should specify the server to install the monitoring master service on.
# The relevant agent will automatically install on all servers in the cluster.
[monitoring]
core
